[Versioning]
version = 1.0.0

[DataProcessing]
raw_data_path = data_lake, datasets, raw_data, challenge_MLE.csv
cleaned_data_path = data_lake, datasets, cleaned_data, cleaned_data.csv
ml_data_path = data_lake, datasets, ml_data, ml_data.csv

target_column = target
datetime_columns = fecha_mesa_epoch, ass_created_at, ass_due_at, ass_unlock_at, ass_lock_at, s_submitted_at, s_graded_at, s_created_at

[Modeling]
model_attr_path = data_lake, models
pipeline_data_path = data_lake, datasets, pipeline_data

balance_train = True
# RandomOverSampler, RandomUnderSampler, SMOTE
balance_method = SMOTE
class_weight = {1: 2, 0: 1}

cutoff = 0.5
algorithms = random_forest, lightgbm, xgboost

# precision, recall, f1_score, roc_auc, accuracy (Accuracy not recommended)
#   - precision = TP / (TP + FP) = TP / Predicted Positive
#   - recall = TP / (TP + FN) = TP / Positive class
#   - f1_score = 2 * precision * recall / (precision + recall)
eval_metric = roc_auc
val_splits = 4
train_test_ratio = 0.2
n_candidates = 10
max_evals = 50
timeout_mins = 60
loss_threshold = -0.997
min_performance = 0.9

[Mlflow]
tracking_url = http://localhost:5050
local_registry = True
local_registry_path = data_lake, model_registry, local_registry.json
artifacts_path = data_lake, artifacts
experiment_name = dev_experiment

[Inference]
# request_url = http://0.0.0.0:5001/predict
request_url = http://model_serving_container_v1.0.0:5000/predict
inference_path = data_lake, inferences

[Updating]
refit_model = False
find_new_shap_values = False
optimize_cutoff = True

[Default]
raw_df = None
save = True
course_name = None
user_uuids = None
particion = None
pick_random = False
